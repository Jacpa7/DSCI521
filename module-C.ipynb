{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 1: Textual feature extraction and numerical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module C _(35 points)_ Similarity of word usage across a document\n",
    "\n",
    "Here we'll be building up some code to discover how different terms are utilized similarly across a document. For this, our first task will be to create a word frequency counting function.\n",
    "\n",
    "__C1.__ _(12 points)_ Define a function called `count_words(paragraph, pos = True, lemma = True)` that `return`s a `Counter()` called `frequency`. In `frequency`, each key will consist of a `heading = (text, tag)`, where `text` contains the `word.text` attribute from `spacy` if `lemma = False`, and `word.lemma_` attribute if `True`. Similarly, `tag` should be left empty as `\"\"` if `pos = False` and otherwise contain `word.pos_`. The `Counter()` should simply contain the number of times each `heading` is observed in the `paragraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import csv\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from previous part, this function loads the paragraphs of the book:\n",
    "def load_book(book_id):\n",
    "    id_string = str(book_id)\n",
    "    text_file = open(\"./data/books/\"+id_string+\".txt\", \"r\")\n",
    "    booktext = text_file.read()\n",
    "    paragraphs = booktext.split('\\n\\n')\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = load_book(84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(paragraph, pos, lemma):\n",
    "    frequency = Counter()\n",
    "    doc = nlp(paragraph)\n",
    "    #heading = (\"word.lemma\" if lemma else \"word.text\", \"tag\" if pos else \"pos off\")\n",
    "    #frequency[heading] = \"count\"\n",
    "    for eachword in doc:\n",
    "        if pos:\n",
    "            if lemma:\n",
    "                frequency[(eachword.lemma_, eachword.pos_)] += 1\n",
    "            else:\n",
    "                frequency[(eachword.text, eachword.pos_)] += 1\n",
    "        else:\n",
    "            if lemma:\n",
    "                frequency[(eachword.lemma_, \"\")] += 1\n",
    "            else:\n",
    "                frequency[(eachword.text, \"\")] += 1\n",
    "\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will rejoice to hear that no disaster has accompanied the\n",
      "commencement of an enterprise which you have regarded with such evil\n",
      "forebodings.  I arrived here yesterday, and my first task is to assure\n",
      "my dear sister of my welfare and increasing confidence in the success\n",
      "of my undertaking.\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('\\n', 'SPACE'): 4, ('-PRON-', 'ADJ'): 4, ('-PRON-', 'PRON'): 3, ('of', 'ADP'): 3, ('to', 'PART'): 2, ('have', 'VERB'): 2, ('the', 'DET'): 2, ('.', 'PUNCT'): 2, ('and', 'CCONJ'): 2, ('will', 'VERB'): 1, ('rejoice', 'VERB'): 1, ('hear', 'VERB'): 1, ('that', 'ADP'): 1, ('no', 'DET'): 1, ('disaster', 'NOUN'): 1, ('accompany', 'VERB'): 1, ('commencement', 'NOUN'): 1, ('an', 'DET'): 1, ('enterprise', 'NOUN'): 1, ('which', 'ADJ'): 1, ('regard', 'VERB'): 1, ('with', 'ADP'): 1, ('such', 'ADJ'): 1, ('evil', 'ADJ'): 1, ('foreboding', 'NOUN'): 1, (' ', 'SPACE'): 1, ('arrive', 'VERB'): 1, ('here', 'ADV'): 1, ('yesterday', 'NOUN'): 1, (',', 'PUNCT'): 1, ('first', 'ADJ'): 1, ('task', 'NOUN'): 1, ('be', 'VERB'): 1, ('assure', 'VERB'): 1, ('dear', 'ADJ'): 1, ('sister', 'NOUN'): 1, ('welfare', 'NOUN'): 1, ('increase', 'VERB'): 1, ('confidence', 'NOUN'): 1, ('in', 'ADP'): 1, ('success', 'NOUN'): 1, ('undertaking', 'NOUN'): 1})\n"
     ]
    }
   ],
   "source": [
    "print(count_words(paragraphs[9], pos = True, lemma = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(count_words(paragraphs[9], pos = True, lemma = True)[('of', 'ADP')]) #which is correct :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C2.__ _(8 pts)_ Next, define a function called `book_TDM(book_id, pos = True, lemma = True)` and copy into it the TDM-producing code from __Section 2.1.5.1__ of the lecture notes, now `return`-ing `TDM` and `all_words`. Once copied, modify this function to call `count_words` appropriately, now passing through the user of `book_TDM`'s specified `lemma` and `pos` arguments.\n",
    "\n",
    "To provde your code's function, process `book_id = 84` with both of `pos = True` and `lemma = True` and print out the `TDM`'s `.shape` attribute and the first ten terms in `all_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_TDM(book_id, pos, lemma):\n",
    "    id_string = str(book_id)\n",
    "    text_file = open(\"./data/books/\"+id_string+\".txt\", \"r\")\n",
    "    text = text_file.read()\n",
    "    doc = nlp(text)\n",
    "    ## the 'master' set, keeps track of the words in all documents\n",
    "    all_words = set()\n",
    "\n",
    "    ## store the word frequencies by book\n",
    "    all_doc_frequencies = {}\n",
    "\n",
    "    ## loop over the sentences\n",
    "    for j, sentence in enumerate(doc.sents):\n",
    "        frequency = count_words(sentence.text, pos, lemma) \n",
    "        #print(frequency)\n",
    "        all_doc_frequencies[j] = frequency\n",
    "        doc_words = set(frequency.keys())\n",
    "        all_words = all_words.union(doc_words)\n",
    "\n",
    "    ## create a matrix of zeros: (words) x (documents)\n",
    "    TDM = np.zeros((len(all_words),len(all_doc_frequencies)))\n",
    "    ## fix a word ordering for the rows\n",
    "    all_words = sorted(list(all_words))\n",
    "\n",
    "    ## loop over the (sorted) document numbers and (ordered) words; fill in matrix\n",
    "    for j in all_doc_frequencies:\n",
    "        for i, word in enumerate(all_words):\n",
    "            TDM[i,j] = all_doc_frequencies[j][word]\n",
    "    return (TDM, all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function works with the whole book, so it takes a while for it to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM_84, all_words_84 = book_TDM(84, pos = True, lemma = True) #this processes TDM for the whole book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6151, 3470)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDM_84.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 'SPACE'),\n",
       " ('\\n\\n', 'SPACE'),\n",
       " ('\\n\\n  ', 'SPACE'),\n",
       " ('\\n\\n    ', 'SPACE'),\n",
       " ('\\n\\n     ', 'SPACE'),\n",
       " ('\\n\\n               ', 'SPACE'),\n",
       " ('\\n\\n                    ', 'SPACE'),\n",
       " ('\\n\\n                                                ', 'SPACE'),\n",
       " ('\\n  ', 'SPACE'),\n",
       " ('\\n   ', 'SPACE')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_84[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yield', 'VERB'),\n",
       " ('yon', 'NOUN'),\n",
       " ('you', 'PRON'),\n",
       " ('young', 'ADJ'),\n",
       " ('youngster', 'NOUN'),\n",
       " ('your', 'ADJ'),\n",
       " ('yourself', 'NOUN'),\n",
       " ('youth', 'NOUN'),\n",
       " ('youthful', 'ADJ'),\n",
       " ('zeal', 'NOUN')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_84[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C3.__ _(8 pts)_ Next, your job is to define two functions. The first is `sim(u,v)`, which shoud take two arbitrary numeric vectors and compute/output the cosine similarity, as described in __Section 1.1.2.10__.  \n",
    "\n",
    "The second function is `term_sims(i, TDM)`, which should utilize the first function (`sim`) to output a list of cosine similarity values between the word/row `i` and all others (rows) in the `TDM`. \n",
    "\n",
    "Note: each of these functions can be straightforwardly completed using a single line of code! Exhibit your knowledge of comprehensions and vectorization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(u,v): return u.dot(v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_sims(i, TDM): return [sim(TDM[i,], TDM[x,]) for x in range(TDM.shape[0]) if x!=i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6150"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(term_sims(2, TDM_84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.025026082427542392,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.020117019055664216,\n",
       " 0.0,\n",
       " 0.06468462273531508,\n",
       " 0.02643505285727147,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_sims(1000, TDM_84)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C4.__ _(7 pts)_ Finally, your goal now is to a write function, `most_similar(term, terms, TDM, top = 25)`, that utilizes `term_sims` to output a sorted list of the `top = N` terms most similar to one specified (`term`). The output data type should be a list of lists, with each inner list representing information for a similar term as: `[row_ix, similarity, term]`. Once complete, prove your function's utility on a `TDM` produced for `book_id = 84` and exhibit the top 25 similar terms to both of `('monster', 'NOUN')` and `('beautiful', 'ADJ')`.\n",
    "\n",
    "Once computation is complete, comment on the ordered results returned in the markdown cell below. Do you think the algorithm is exhibiting sensible result? What would you do to improve?\n",
    "\n",
    "\\[Hint: to locate the row containing the term of interest, utilize the list `.index()` method in application to the `terms` argument.\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>One problem with this method is that we are counting new lines, spaces, etc. as words as we can see in the beginning of the sorted list of words. and they contribute to the similarity calculations. One way to improve is that to exclude those words from the computations. Also, it maybe useful to add the stop words and count them as a limiting criterion.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(term, terms, TDM, top):\n",
    "    sim_info = []\n",
    "    i = terms.index(term)\n",
    "    sims = term_sims(i, TDM)\n",
    "    for j, eachsim in enumerate(sims):\n",
    "        sim_info.append([j, eachsim, terms[j]])\n",
    "    sim_data = sorted(sim_info, key=lambda x: x[1], reverse = True)\n",
    "    output = sim_data[:top]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "monster_similars = most_similar(('monster', 'NOUN'), all_words_84, TDM_84, top = 25)\n",
    "beautiful_similars = most_similar(('beautiful', 'ADJ'), all_words_84, TDM_84, top = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity info for the word monster as a noun: \n",
      "\n",
      "[[467, 0.17407765595569785, ('asseveration', 'NOUN')],\n",
      " [659, 0.17407765595569785, ('besiege', 'VERB')],\n",
      " [1225, 0.17407765595569785, ('convulse', 'VERB')],\n",
      " [1547, 0.17407765595569785, ('detested', 'ADJ')],\n",
      " [1588, 0.17407765595569785, ('dim', 'NOUN')],\n",
      " [1645, 0.17407765595569785, ('disown', 'VERB')],\n",
      " [2029, 0.17407765595569785, ('existence', 'VERB')],\n",
      " [2295, 0.17407765595569785, ('forehead', 'NOUN')],\n",
      " [2387, 0.17407765595569785, ('furiously', 'ADV')],\n",
      " [2925, 0.17407765595569785, ('inevitable', 'ADJ')],\n",
      " [3117, 0.17407765595569785, ('jeer', 'NOUN')],\n",
      " [3206, 0.17407765595569785, ('languish', 'VERB')],\n",
      " [3278, 0.17407765595569785, ('lid', 'NOUN')],\n",
      " [3504, 0.17407765595569785, ('merciless', 'NOUN')],\n",
      " [3544, 0.17407765595569785, ('mirror', 'NOUN')],\n",
      " [3606, 0.17407765595569785, ('mortal', 'NOUN')],\n",
      " [4320, 0.17407765595569785, ('proportionably', 'ADV')],\n",
      " [4418, 0.17407765595569785, ('rational', 'ADJ')],\n",
      " [4808, 0.17407765595569785, ('scream', 'NOUN')],\n",
      " [5650, 0.17407765595569785, ('unalli', 'VERB')],\n",
      " [6025, 0.1484539238050411, ('wholly', 'ADV')],\n",
      " [2213, 0.1315903389919538, ('finger', 'NOUN')],\n",
      " [5162, 0.1315903389919538, ('stars', 'PROPN')],\n",
      " [3265, 0.12506107989614448, ('let', 'VERB')],\n",
      " [645, 0.12309149097933272, ('beneficial', 'ADJ')]]\n",
      "\n",
      "Similarity info for the word beautiful as an adjective: \n",
      "\n",
      "[[4261, 0.26726124191242434, ('pride', 'NOUN')],\n",
      " [4592, 0.26726124191242434, ('research', 'NOUN')],\n",
      " [55, 0.1889822365046136, ('27th', 'NOUN')],\n",
      " [183, 0.1889822365046136, ('adored', 'ADJ')],\n",
      " [217, 0.1889822365046136, ('affright', 'NOUN')],\n",
      " [278, 0.1889822365046136, ('alluring', 'ADJ')],\n",
      " [309, 0.1889822365046136, ('amid', 'ADP')],\n",
      " [516, 0.1889822365046136, ('aught', 'NOUN')],\n",
      " [585, 0.1889822365046136, ('bat', 'NOUN')],\n",
      " [661, 0.1889822365046136, ('best', 'ADV')],\n",
      " [832, 0.1889822365046136, ('candour', 'NOUN')],\n",
      " [1019, 0.1889822365046136, ('coast', 'NOUN')],\n",
      " [1055, 0.1889822365046136, ('common', 'ADJ')],\n",
      " [1371, 0.1889822365046136, ('de', 'PROPN')],\n",
      " [1794, 0.1889822365046136, ('ecstasy', 'VERB')],\n",
      " [1811, 0.1889822365046136, ('elapse', 'VERB')],\n",
      " [1850, 0.1889822365046136, ('empty', 'ADJ')],\n",
      " [1897, 0.1889822365046136, ('enlightened', 'ADJ')],\n",
      " [1921, 0.1889822365046136, ('entrance', 'VERB')],\n",
      " [2058, 0.1889822365046136, ('exquisite', 'ADJ')],\n",
      " [2364, 0.1889822365046136, ('fringe', 'VERB')],\n",
      " [2611, 0.1889822365046136, ('harsh', 'ADJ')],\n",
      " [2647, 0.1889822365046136, ('heavenly', 'ADJ')],\n",
      " [2727, 0.1889822365046136, ('host', 'NOUN')],\n",
      " [3072, 0.1889822365046136, ('invade', 'VERB')]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity info for the word monster as a noun: \\n\")\n",
    "pprint(monster_similars)\n",
    "print(\"\\nSimilarity info for the word beautiful as an adjective: \\n\")\n",
    "pprint(beautiful_similars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
