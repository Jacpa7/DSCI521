{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 4: Machine learning and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kiana Montazeri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module A _(49 pts)_ Exploring dimensionality reduction through image classification\n",
    "__Data.__ For this module we'll be working with the MNIST dataset of 28 x 28 black and white pixel-images of hand-written numbers, which was put together some folks at NYU, Google Labs, and Microsoft Research:\n",
    "\n",
    "- http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "For simplicity, we won't be access these data from the producers' site, since it's presented in a compressed format that requires special code (there's a [python module](https://pypi.org/project/python-mnist/), if you're interested).\n",
    "\n",
    "The MNIST dataset has become extremely important to the ML community over the years as a standard dataset for image classification that has a very clear 'true' label set (unlike others, e.g., like sentiment classification in __Module B__). For ML in general, it's very important to have datasets over which the community can compare results, hence compare algorithms without the uncertainty of data variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries in use:\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as  pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "# if it's not working: update sklearn using this command >> pip install scikit-learn --upgrade\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A1.__ _(5 pts_) To load the MNIST data, we're actually going to be able to use `sklearn`! Since the data have become such a standard, and `sklearn` is _the_ standard ML library it has an `sklearn.datasets.fetch_openml()` method that we can use to download datasets:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html\n",
    "\n",
    "So, using this method, load the MNIST data as `mnist`, setting `name='mnist_784'`, `version=1`, and `cache=True` as arguments. Note: the latter two arguments 1) standardize our verson and 2) store the downloaded data on your local machine. If you're ever curious, the data should wind up in `'~/scikit_learn_data/'`.\n",
    "\n",
    "Once you've loaded `mnist`, inspect its `.data` attribute. In particular, print and discuss the size and type (number of features and their values) of a a single record `mnist.data[i]` for some `i`, and describe in the response box below how these data appear represent a black and white image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Data consists of $70000$ handwritten $28 \\times 28$ pixel images. Aach pixel of the image is represented by a value between 0 and 255, where 0 is black, 255 is white and anything in between is a different shade of grey.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(name='mnist_784', version=1, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  51., 159., 253., 159.,  50.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        48., 238., 252., 252., 252., 237.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  54., 227., 253., 252., 239., 233.,\n",
       "       252.,  57.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10.,  60.,\n",
       "       224., 252., 253., 252., 202.,  84., 252., 253., 122.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0., 163., 252., 252., 252., 253., 252., 252.,\n",
       "        96., 189., 253., 167.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51., 238.,\n",
       "       253., 253., 190., 114., 253., 228.,  47.,  79., 255., 168.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  48., 238., 252., 252., 179.,  12.,  75., 121.,\n",
       "        21.,   0.,   0., 253., 243.,  50.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  38., 165., 253.,\n",
       "       233., 208.,  84.,   0.,   0.,   0.,   0.,   0.,   0., 253., 252.,\n",
       "       165.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   7., 178., 252., 240.,  71.,  19.,  28.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0., 253., 252., 195.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  57., 252., 252.,\n",
       "        63.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 253.,\n",
       "       252., 195.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0., 198., 253., 190.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 255., 253., 196.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  76., 246., 252.,\n",
       "       112.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       253., 252., 148.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  85., 252., 230.,  25.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   7., 135., 253., 186.,  12.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  85., 252.,\n",
       "       223.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   7., 131.,\n",
       "       252., 225.,  71.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  85., 252., 145.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  48., 165., 252., 173.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  86.,\n",
       "       253., 225.,   0.,   0.,   0.,   0.,   0.,   0., 114., 238., 253.,\n",
       "       162.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  85., 252., 249., 146.,  48.,  29.,\n",
       "        85., 178., 225., 253., 223., 167.,  56.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        85., 252., 252., 252., 229., 215., 252., 252., 252., 196., 130.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  28., 199., 252., 252., 253.,\n",
       "       252., 252., 233., 145.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  25., 128., 252., 253., 252., 141.,  37.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2.__ _(7 pts)_ Spoiler alert! Each image record is represented as an integer `np.array` of size `(784,)` (you still have to show that above). Each integer represents a pixel, with shade of gray encoded as an intensity from 0&ndash;255, i.e., white&ndash;black. Notice how 28 divides 784 evenly? That's because the rows of the image have been 'stacked' into a single row of data!\n",
    "\n",
    "Since we're going to want to visualize these images, e.g., to check and see why our algorithm fails where. But in order to do this, we're going to need to build a helper function that pre-processes a given record into it's representation as a `(28,28)` matrix of rows. In particular, write a function called `unstack(x)` that accepts any image/record `x` and outputs an `np.array()` of size `(28, 28)` containing the pixel values such that the top row holds pixels 0&ndash;27, the second contains pixels 28&ndash;55, etc.\n",
    "\n",
    "[Hint. `numpy.array`s have a `.reshape()` method that can do the unstacking for you, check it out!]\n",
    "\n",
    "When complete, print the result from application to the same record as observed in __A1__. Can you see any shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstack(x):\n",
    "    x_unstacked = x.reshape(28, 28)\n",
    "    return x_unstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data_1_unstacked = unstack(mnist.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data_1_unstacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,  51., 159., 253., 159.,  50.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,  48., 238., 252., 252., 252., 237.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,  54., 227., 253., 252., 239., 233., 252.,  57.,   6.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         10.,  60., 224., 252., 253., 252., 202.,  84., 252., 253., 122.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        163., 252., 252., 252., 253., 252., 252.,  96., 189., 253., 167.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  51.,\n",
       "        238., 253., 253., 190., 114., 253., 228.,  47.,  79., 255., 168.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  48., 238.,\n",
       "        252., 252., 179.,  12.,  75., 121.,  21.,   0.,   0., 253., 243.,\n",
       "         50.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  38., 165., 253.,\n",
       "        233., 208.,  84.,   0.,   0.,   0.,   0.,   0.,   0., 253., 252.,\n",
       "        165.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   7., 178., 252., 240.,\n",
       "         71.,  19.,  28.,   0.,   0.,   0.,   0.,   0.,   0., 253., 252.,\n",
       "        195.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  57., 252., 252.,  63.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 253., 252.,\n",
       "        195.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0., 198., 253., 190.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 255., 253.,\n",
       "        196.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  76., 246., 252., 112.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 253., 252.,\n",
       "        148.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  85., 252., 230.,  25.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   7., 135., 253., 186.,\n",
       "         12.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  85., 252., 223.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   7., 131., 252., 225.,  71.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  85., 252., 145.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,  48., 165., 252., 173.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  86., 253., 225.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0., 114., 238., 253., 162.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  85., 252., 249., 146.,  48.,\n",
       "         29.,  85., 178., 225., 253., 223., 167.,  56.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  85., 252., 252., 252., 229.,\n",
       "        215., 252., 252., 252., 196., 130.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  28., 199., 252., 252., 253.,\n",
       "        252., 252., 233., 145.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,  25., 128., 252., 253.,\n",
       "        252., 141.,  37.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data_1_unstacked #it doesnt fit in the whole area. I still cant see the number!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3.__ _(3 pts)_ Now that we've got the ability to unstack our images, let's visualize them. In particular, write a function called `show_pixels(px)` that \n",
    "\n",
    "1. accepts an unstacked collection of pixels, `px`, and\n",
    "2. utilizes `plt.imshow()` on `px` with an appropriate `cmap` argument to express the pictures.\n",
    "\n",
    "When complete, exhibit your function's output on your same chosen example from __A1&ndash;A3__. You should definitely be able to tell what number the picture represents now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pixels(px):\n",
    "    unstacked = unstack(px)\n",
    "    im = plt.imshow(unstacked, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pixels(mnist.data[1]) #it's zero :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A4.__ _(2 pts)_ Now let's start our analysis by exploring how PCA's ability to reduce dimensionality can be understood as a way to _reduce the resolutiuon of data_. So, start by importing `PCA` from `sklearn.decomposition`, and create three instances: `pca5`, `pca50`, and `pca75`, which will reduce dimensionality to explain only 5%, 50%, and 75% of the dataset's variance, respectively. Then for each, run `.fit(mnist.data)` to train a `PCA` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.75, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## perform PCA on our data\n",
    "pca5 = PCA(n_components=0.05)\n",
    "pca50 = PCA(n_components=0.5)\n",
    "pca75 = PCA(n_components=0.75)\n",
    "pca5.fit(mnist.data)\n",
    "pca50.fit(mnist.data)\n",
    "pca75.fit(mnist.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A5.__ _(4 pts)_ Now, using your `pca5`, `pca50`, and `pca75` models, create a function called `deresolve(x, pca)` that accepts an image/record `x` from `mnist.data` and a trained `pca` model to construct an approximation, `deresolved` image. This can be done using `pca.transform()` to construct the dimensionally `reduced` representation of an image as output, which can then be passed to the `pca.inverse_transform()` function to create the `deresolved` pixels, which should be `return`ed by the function.\n",
    "\n",
    "When complete, pass the output of your function's application to your chosen example image to your `show_pixels()` function for each of the `pca5`, `pca50`, and `pca75` models and comment in the response box below how these pca models are abstracting the key features of the data, i.e., shapes of numbers.\n",
    "\n",
    "Note: the `pca.transform()` method expects an array of records as input, so if your chosen example image is `mnist.data[i]`, then you'll have to pass `[mnist.data[i]]` to the `.transform()` function to get it to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Each model uses only a percentile of the data to train. The overall picture is clear but the clarirt of the number zero increases with more data in use for the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deresolve(x, pca):\n",
    "    reduced = pca.transform([x])\n",
    "    deresolved_x = pca.inverse_transform(reduced)\n",
    "    return deresolved_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "deresolved_5 = deresolve(mnist.data[1], pca5)\n",
    "deresolved_50 = deresolve(mnist.data[1], pca50)\n",
    "deresolved_75 = deresolve(mnist.data[1], pca75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 784), (1, 784), (1, 784))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deresolved_5.shape, deresolved_50.shape, deresolved_75.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pixels(deresolved_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pixels(deresolved_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_pixels(deresolved_75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6.__ _(3 pts)_ Before we get too deep in with analysis, it would be a good idea to separate our data by training and testing. So, use `train_test_split` from `sklearn.model_selection` to split up the data, and store them as `train`, `test`, `train_labels`, `test_labels`. Be sure to select an appropriate `test_size` and `random_state` for the split, and justify these settings in the response box below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Random state will make the process reproducable by setting a random seed. The split size is 80% and 20%. We can have 5 fold cross validation.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A7.__ _(3 pts)_ Now that we have access to `sklearn` we should use it for all it's worth! We'll do well to standardize our data to move forward with classificaition, but since `sklearn` has the `StandardScaler` function from `sklearn.processing` we can use that! In particular, import `StandardScalar` and `.fit()` a model to the `train` portion of the data set. Then, use the `.transform()` method on the resulting trained standardization object to produce standardized versions of `train` and `test`, calling them `train_std` and `test_std`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalertrain = StandardScaler().fit(train)\n",
    "scalertest = StandardScaler().fit(test)\n",
    "train_std = scalertrain.transform(train)\n",
    "test_std = scalertrain.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A8.__ _(3 pts)_ Now it's time to apply PCA for the purposes of classification! However, we'll have to re-train a PCA model _on the `train`ing set_ (only) for our desired variance percentage that we'd like to retain. So, write a function called `reduce(percentage)` that:\n",
    "\n",
    "1. creates a `PCA` instance for the desired `percentage` variance explained,\n",
    "2. runs `.fit()` on the standardized `train`ing data `train_std`,\n",
    "3. applies the resulting `.transform()` method to both `train_std` and `test_std`, returning the results.\n",
    "\n",
    "When complete, apply this function using a `percentage` for `95%` of the variance explained and store the outputs as `train_std_pca`, and `test_std_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(percentage):\n",
    "    pca = PCA(n_components=percentage)\n",
    "    pca.fit(train_std)\n",
    "    train_std_pca = pca.transform(train_std)\n",
    "    test_std_pca = pca.transform(test_std)\n",
    "    return train_std_pca, test_std_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std_pca, test_std_pca = reduce(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A9.__ _(7 pts)_ Now it's time to train a classifier. In particular, your job here is to write a function called `train_model(train_dat, train_lbl)` that accepts training data (however pre-processed) with labels and trains. To do this, utilize `LogisticRegression` from `sklearn.linear_model`, but be sure to utilize the following arguments for initializing the model's instance: \n",
    "- `solver = 'lbfgs'`,\n",
    "- `max_iter=1000`, and \n",
    "- `multi_class='ovr'`. \n",
    "\n",
    "Once you've initialized the logistic regression model, apply the `.fit()` method to the data input to your function and return the resulting classifier object. When this function is complete, apply it to the dimensionally reduced `train_std_pca` data output in __A8__ and store the resulting classifier as `clf`.\n",
    "\n",
    "Note: The first two arguments to `LogisticRegression` specify the `solver` being used to optimize the regression (as in __Chapter 7__), and the maximum number of iterations to optimize over. However, the `multi_class='ovr'` argument is new to us! In particular, since we're trying to pick one from 10 discrete labels, i.e., `'0'`, `'1'`, `'2'`, etc., for each input record this is called a _multi-class_ classification problem. Since logistic regression isn't built for this kind of application there has to be a way to modify/enhance the algorithm to support it. As it turns out there's one way in which we can modify any binary classifier into a multi-class classifier, which is what `multi_class='ovr'` does. In particular, `'ovr'` indicates the _one vs. rest_ strategy which trains as many classifiers as there are possible outcomes in the labels. Each of the trained classifiers is trained to positively predict a given label, e.g., `'7'`, and negatively predict against all other, e.g., `not '7'`. All classifiers (10 for us) are then applied to each test record, and the classifier that offers the best positive prediction probability is then corresponds to the predicted label for the record. That's it! While the one vs. rest procedure is _general_ in its ability to convert a binary classifier into a multi-class classifier, it is a limited framework for multi-class classification, and more-precise models are generally those that do best. To see a derived multi-class logistic regression model, check out [Multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), which can be applied by setting `solver = 'multinomial'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dat, train_lbl):\n",
    "    LogReg = LogisticRegression(solver='lbfgs', max_iter = 1000, multi_class='ovr')\n",
    "    # Create an instance of Logistic Regression Classifier and fit the data.\n",
    "    clf = LogReg.fit(train_dat, train_lbl)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = train_model(train_std_pca, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A10.__ _(4 pts)_ Now that we have a trained classifier it's time to explore its application to the test set. For this, utilize the `clf.predict()` method on the `test_std_pca` data, storing the result as `test_labels_predicted`. Since this is _not_ a binary classification problem, we likewise can't use binary classification evaluation metrics (i.e., precision, recall, and $F_1$), and so have to fall back on accuracy. So import `accuracy_score` from `sklearn.metrics` and apply it to the pair of `test_labels_predicted` and `test_labels` and report your accuracy. When this is done, go on to compare the first 25 values of `test_labels_predicted` and `test_labels` to see how the model did.  Then, in the response box below discuss\n",
    "1. The types of errors being made by the model, and\n",
    "2. how this output differs from the context of a binary classification problem.\n",
    "\n",
    "Note: for 2, the point to discuss is not over prediction of 0s and 1s, but rather the number of different possible labels that the algorithm is responsible for predicting. This is important, as logistic regression as we've discussed is technically only set up to support binary classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>[Refer to here!](https://en.wikipedia.org/wiki/Uncertainty_quantification) Error come from either intrinsic uncertainty or epistemic uncertainty. Intrinsic uncertainty is when, for data in the training set, data points with very similar feature vectors have targets with substantially different values. Collecting more data will never help a model reduce this type of uncertainty. Epistemic uncertainty is also known as systematic uncertainty, and is due to things one could in principle know but doesn't in practice. This may be because a measurement is not accurate, because the model neglects certain effects, or because particular data has been deliberately hidden.\n",
    "<br><br>\n",
    "Since we have more than two classes for the output, the accuracy of random selection would be less than 50 % of times i.e only 10 % of the times random slection will result in correct prediction.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_predicted = clf.predict(test_std_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(test_labels_predicted, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9172142857142858\n"
     ]
    }
   ],
   "source": [
    "print(score) #almost 92 percent true predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8' '4' '8' '7' '7' '0' '6' '2' '7' '4' '3' '9' '9' '8' '2' '5' '9' '1'\n",
      " '7' '8' '0' '0' '0' '3' '6']\n",
      "['8' '4' '8' '7' '7' '0' '6' '2' '7' '4' '3' '9' '9' '8' '2' '5' '9' '1'\n",
      " '7' '8' '0' '0' '0' '3' '6']\n"
     ]
    }
   ],
   "source": [
    "print(test_labels_predicted[:25])\n",
    "print(test_labels[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A11.__ _(8 pts)_ Now that we have our training and testing pipline in place and we've seen what sklearn and logistic regresion can do for image classification, let's explore how `PCA` affects the process. We'll be evaluating how much time each implementation affects the run time, so the first thing you'll want to do is import `datetime`. Following this, write a loop over `portion` values `0.1`, `0.2`, ..., `0.9`. For each `portion`:\n",
    "1. records an initial `datetime.datetime.now()` timestamp as `start`;\n",
    "2. applies your `reduce()` function with the `portion` specified and store the resulting data as `train_std_pca`, `test_std_pca`;\n",
    "3. passes the outputs from 2 to your `train_model()` function and store the classifier as `clf`;\n",
    "4. applies the `clf.predict()` method  to the transformed `test_std_pca` data and store the resulting predictions as `test_labels_predicted`,;\n",
    "5. records a final `datetime.datetime.now()` timestamp as `finish`; and\n",
    "6. `print`s the model `accuracy_score` and execution time (computed as `finish-start`).\n",
    "\n",
    "When this is complete, you should have evaluations for each level of dimensionality reduction provided by PCA. With these in front of you, discuss the effects of PCA on run time and accuracy in the response box below. In particular, at which level of PCA-resolution do you think the best model is produced, considering both accuracy and run time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>With low percentage of data the accuracy is low and with high portions the accuracy and execution time increases. We should find the optimal point where the time is reasonable and the accuracy is high. It seems like the $0.8$ is the most accurate(91%) yet the execution time is still not very long.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4630714285714286 0:00:10.894018\n",
      "0.7121428571428572 0:00:11.027282\n",
      "0.7994285714285714 0:00:09.646877\n",
      "0.8605 0:00:13.527922\n",
      "0.8896428571428572 0:00:13.794598\n",
      "0.8983571428571429 0:00:19.205313\n",
      "0.9081428571428571 0:00:41.803250\n",
      "0.9146428571428571 0:01:04.041237\n",
      "0.9178571428571428 0:01:29.953042\n"
     ]
    }
   ],
   "source": [
    "portion = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "for eachportion in portion:\n",
    "    start = datetime.now()\n",
    "    train_std_pca, test_std_pca = reduce(eachportion)\n",
    "    clf = train_model(train_std_pca, train_labels)\n",
    "    test_labels_predicted = clf.predict(test_std_pca)\n",
    "    finish = datetime.now()\n",
    "    score = accuracy_score(test_labels_predicted, test_labels)\n",
    "    exe_time = finish-start\n",
    "    print(score, exe_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
